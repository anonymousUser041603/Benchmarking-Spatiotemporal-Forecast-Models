{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhIZ4EDS7zX-"
      },
      "source": [
        "#Benchmark of Spatiotemporal Graph Neural Networks for Short-Term Load Forecasting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install basic dependencies."
      ],
      "metadata": {
        "id": "qmjdS2L_Rg2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here's a single command to uninstall *all* of the packages you installed for STGNN and its dependencies:\n",
        "!pip uninstall -y \\\n",
        "    numpy pandas scipy matplotlib plotnine mizani \\\n",
        "    torch torchvision torchaudio torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric \\\n",
        "    pytorch-lightning torch-spatiotemporal \\\n",
        "    gluonts transformers sktime jax jaxlib huggingface-hub hydra-core jaxtyping multiprocess python-dotenv einops dtw-python websockets fsspec gcsfs pyinform optuna h5py typing-extensions wheel\n",
        "!pip install numpy==1.26.4 pandas==2.2.2\n",
        "!pip install --upgrade pandas pandasai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhasZp8-dRkq",
        "outputId": "5e461579-74b4-4e55-c5d2-cb61148a1a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Found existing installation: pandas 2.2.2\n",
            "Uninstalling pandas-2.2.2:\n",
            "  Successfully uninstalled pandas-2.2.2\n",
            "Found existing installation: scipy 1.15.3\n",
            "Uninstalling scipy-1.15.3:\n",
            "  Successfully uninstalled scipy-1.15.3\n",
            "Found existing installation: matplotlib 3.10.0\n",
            "Uninstalling matplotlib-3.10.0:\n",
            "  Successfully uninstalled matplotlib-3.10.0\n",
            "Found existing installation: plotnine 0.14.5\n",
            "Uninstalling plotnine-0.14.5:\n",
            "  Successfully uninstalled plotnine-0.14.5\n",
            "Found existing installation: mizani 0.13.5\n",
            "Uninstalling mizani-0.13.5:\n",
            "  Successfully uninstalled mizani-0.13.5\n",
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Successfully uninstalled torch-2.6.0+cu124\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n",
            "Found existing installation: torchaudio 2.6.0+cu124\n",
            "Uninstalling torchaudio-2.6.0+cu124:\n",
            "  Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-sparse as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-cluster as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-spline-conv as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping pytorch-lightning as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-spatiotemporal as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping gluonts as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: transformers 4.51.3\n",
            "Uninstalling transformers-4.51.3:\n",
            "  Successfully uninstalled transformers-4.51.3\n",
            "\u001b[33mWARNING: Skipping sktime as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: jax 0.5.2\n",
            "Uninstalling jax-0.5.2:\n",
            "  Successfully uninstalled jax-0.5.2\n",
            "Found existing installation: jaxlib 0.5.1\n",
            "Uninstalling jaxlib-0.5.1:\n",
            "  Successfully uninstalled jaxlib-0.5.1\n",
            "Found existing installation: huggingface-hub 0.31.1\n",
            "Uninstalling huggingface-hub-0.31.1:\n",
            "  Successfully uninstalled huggingface-hub-0.31.1\n",
            "\u001b[33mWARNING: Skipping hydra-core as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping jaxtyping as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: multiprocess 0.70.15\n",
            "Uninstalling multiprocess-0.70.15:\n",
            "  Successfully uninstalled multiprocess-0.70.15\n",
            "\u001b[33mWARNING: Skipping python-dotenv as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: einops 0.8.1\n",
            "Uninstalling einops-0.8.1:\n",
            "  Successfully uninstalled einops-0.8.1\n",
            "\u001b[33mWARNING: Skipping dtw-python as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: websockets 15.0.1\n",
            "Uninstalling websockets-15.0.1:\n",
            "  Successfully uninstalled websockets-15.0.1\n",
            "Found existing installation: fsspec 2025.3.2\n",
            "Uninstalling fsspec-2025.3.2:\n",
            "  Successfully uninstalled fsspec-2025.3.2\n",
            "Found existing installation: gcsfs 2025.3.2\n",
            "Uninstalling gcsfs-2025.3.2:\n",
            "  Successfully uninstalled gcsfs-2025.3.2\n",
            "\u001b[33mWARNING: Skipping pyinform as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping optuna as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: h5py 3.13.0\n",
            "Uninstalling h5py-3.13.0:\n",
            "  Successfully uninstalled h5py-3.13.0\n",
            "Found existing installation: typing_extensions 4.13.2\n",
            "Uninstalling typing_extensions-4.13.2:\n",
            "  Successfully uninstalled typing_extensions-4.13.2\n",
            "Found existing installation: wheel 0.45.1\n",
            "Uninstalling wheel-0.45.1:\n",
            "  Successfully uninstalled wheel-0.45.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Unzip tower data zip file."
      ],
      "metadata": {
        "id": "uSxmrQwCRkhz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KElwkaNRWNl6",
        "outputId": "9b2fa6e7-9fbb-43e9-8c86-3c5b017203ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  IoBT All Tower Data.zip\n",
            "   creating: IoBT All Tower Data/\n",
            "  inflating: IoBT All Tower Data/MSA Tower LAT LONG.CSV  \n",
            "  inflating: IoBT All Tower Data/tower10_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower11_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower12_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower13_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower14_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower15_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower18_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower1_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower21_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower22_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower23_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower24_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower25_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower26_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower27_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower28_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower29_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower2_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower30_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower31_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower32_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower36_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower3_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower4_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower5_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower6_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower7_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower8_data_processed_1to9days.csv  \n",
            "  inflating: IoBT All Tower Data/tower9_data_processed_1to9days.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip 'IoBT All Tower Data.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PKz_QO3Zice",
        "outputId": "02b2d156-87e5-483b-98cd-7361e70ee87a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n"
          ]
        }
      ],
      "source": [
        "!ls 'IoBT All Tower Data' | wc -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZGS80py74rg",
        "outputId": "ad49f798-81b9-42ef-f8c1-d5a225828e41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: 4754 rows had invalid DateTime and will be dropped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-0114900fe0a0>:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['HourlyDateTime'] = data['DateTime'].dt.round('H')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['tower13', 'tower29', 'tower4', 'tower18', 'tower24', 'tower31', 'tower2', 'tower22', 'tower12', 'tower25', 'tower14', 'tower10', 'tower30', 'tower6', 'tower8', 'tower32', 'tower5', 'tower28', 'tower7', 'tower9', 'tower26', 'tower15', 'tower11', 'tower36', 'tower21', 'tower3', 'tower23', 'tower1', 'tower27'])\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "all_towers = []\n",
        "root = 'IoBT All Tower Data'\n",
        "features = [\"HourlyDateTime\", \"Temperature_2m\", \"BarometricPressure\", \"RelativeHumidity_2m\", \"Rainfall\"]\n",
        "\n",
        "def group_data_hourly(data):\n",
        "    # Clean and format the 'Date' column (remove trailing \".0\")\n",
        "    data['Date'] = data['Date'].astype(str).str.replace(r'\\.0$', '', regex=True)\n",
        "\n",
        "    # Clean and format 'Time' to remove any fractional part and ensure it's zero-padded correctly\n",
        "    data['Time'] = data['Time'].apply(lambda x: '{:04}'.format(int(float(x))))\n",
        "\n",
        "    # Combine 'Date' and formatted 'Time' columns correctly to create a datetime column\n",
        "    data['DateTime'] = pd.to_datetime(data['Date'] + ' ' + data['Time'], format='%Y%m%d %H%M', errors='coerce')\n",
        "\n",
        "    # Remove rows that could not be converted\n",
        "    if data['DateTime'].isnull().any():\n",
        "        num_invalid = data['DateTime'].isnull().sum()\n",
        "        print(f\"Warning: {num_invalid} rows had invalid DateTime and will be dropped.\")\n",
        "        data = data.dropna(subset=['DateTime'])\n",
        "\n",
        "    # Round datetime to the nearest hour\n",
        "    data['HourlyDateTime'] = data['DateTime'].dt.round('H')\n",
        "\n",
        "    # Group by hourly datetime and aggregate using mean for specified columns\n",
        "    hourly_aggregated_data = data.groupby('HourlyDateTime').agg({\n",
        "        'BatteryVoltage': 'mean',\n",
        "        'PanelTemperature': 'mean',\n",
        "        'BarometricPressure': 'mean',\n",
        "        'Rainfall': 'mean',\n",
        "        'Temperature_2m': 'mean',\n",
        "        'RelativeHumidity_2m': 'mean',\n",
        "        'Temperature_10m': 'mean',\n",
        "        'SolarRadiation_W/m2': 'mean',\n",
        "        'SolarRadiation_kW/m2': 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    return hourly_aggregated_data\n",
        "towerMap = dict()\n",
        "allFiles = []\n",
        "# Process each CSV file\n",
        "for file in os.listdir(root):\n",
        "    if file.endswith(\"1to9days.csv\"):\n",
        "        allFiles.append(file)\n",
        "        towerName = file.split(\"_\")[0]\n",
        "        if towerName not in towerMap:\n",
        "          towerMap[towerName] = []\n",
        "        file_path = os.path.join(root, file)\n",
        "        tower = pd.read_csv(file_path)\n",
        "        tower_aggregated = group_data_hourly(tower)\n",
        "        towerMap[towerName] = tower_aggregated[features]\n",
        "        all_towers.append(tower_aggregated[features])\n",
        "\n",
        "# Optionally, combine all towers into one DataFrame\n",
        "combined_towers_df = pd.concat(all_towers, ignore_index=True)\n",
        "print(towerMap.keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw9awulmefgM",
        "outputId": "999fb4ea-c92b-46c6-f20f-00ebda835712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['tower13', 'tower4', 'tower18', 'tower24', 'tower31', 'tower2', 'tower12', 'tower25', 'tower14', 'tower10', 'tower30', 'tower6', 'tower8', 'tower32', 'tower5', 'tower28', 'tower7', 'tower9', 'tower26', 'tower15', 'tower11', 'tower21', 'tower3', 'tower23', 'tower27'])\n",
            "25\n"
          ]
        }
      ],
      "source": [
        "# Iterate over a copy of the dictionary items\n",
        "for name, val in list(towerMap.items()):\n",
        "    # If either dimension does not meet the expected value, delete it\n",
        "    if val.shape[0] != 217 or val.shape[1] != 5:\n",
        "      for files in allFiles:\n",
        "        if name+\"_\" in files:\n",
        "            del towerMap[name]\n",
        "            del allFiles[allFiles.index(files)]\n",
        "            print(f\"Deleting tower {name} with shape {val.shape} and path {files}\")\n",
        "\n",
        "# Get the number of towers remaining in towerMap\n",
        "print(towerMap.keys())\n",
        "print(len(allFiles))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Function to group data by a sampling rate 'X' (in mins)"
      ],
      "metadata": {
        "id": "qTLaAbsFRsml"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HjTw9aczoqq3"
      },
      "outputs": [],
      "source": [
        "def group_data_by_interval(data, round_interval, datetime_col_name):\n",
        "    features = [datetime_col_name, \"Temperature_2m\", \"BarometricPressure\", \"RelativeHumidity_2m\", \"Rainfall\"]\n",
        "\n",
        "    \"\"\"\n",
        "    Aggregate sensor data based on a specified time rounding interval.\n",
        "\n",
        "    Parameters:\n",
        "      data (pd.DataFrame): Source data including 'Date', 'Time', optionally 'Timestamp'.\n",
        "      round_interval (str): Rounding frequency (e.g., '15min', '45min', '5min').\n",
        "      datetime_col_name (str): Name for the new rounded datetime column.\n",
        "\n",
        "    Returns:\n",
        "      pd.DataFrame: Aggregated data grouped by rounded datetime.\n",
        "    \"\"\"\n",
        "\n",
        "    # Clean 'Date' (remove trailing \".0\")\n",
        "    data['Date'] = data['Date'].astype(str).str.replace(r'\\.0$', '', regex=True)\n",
        "\n",
        "    # Ensure 'Time' has no fractional part and is correctly zero-padded\n",
        "    data['Time'] = data['Time'].apply(lambda x: '{:04}'.format(int(float(x))))\n",
        "\n",
        "    # Combine 'Date' and cleaned 'Time' into datetime\n",
        "    data['DateTime'] = pd.to_datetime(\n",
        "        data['Date'] + ' ' + data['Time'],\n",
        "        format='%Y%m%d %H%M',\n",
        "        errors='coerce'\n",
        "    )\n",
        "\n",
        "    # Drop rows with invalid datetime\n",
        "    if data['DateTime'].isnull().any():\n",
        "        num_invalid = data['DateTime'].isnull().sum()\n",
        "        print(f\"Warning: {num_invalid} rows had invalid DateTime and will be dropped.\")\n",
        "        data = data.dropna(subset=['DateTime'])\n",
        "\n",
        "    # Add seconds after hour from 'Timestamp' column if present\n",
        "    if 'Timestamp' in data.columns:\n",
        "        data['SecondsAfterHour'] = (data['Timestamp'].astype(float) % 3600)\n",
        "        data['DateTime'] += pd.to_timedelta(data['SecondsAfterHour'], unit='s')\n",
        "\n",
        "    # Round datetime to specified interval and create new datetime column\n",
        "    data[datetime_col_name] = data['DateTime'].dt.round(round_interval)\n",
        "\n",
        "    # Group by rounded datetime and aggregate measurements\n",
        "    aggregated_data = data.groupby(datetime_col_name).agg({\n",
        "        'BatteryVoltage': 'mean',\n",
        "        'PanelTemperature': 'mean',\n",
        "        'BarometricPressure': 'mean',\n",
        "        'Rainfall': 'mean',\n",
        "        'Temperature_2m': 'mean',\n",
        "        'RelativeHumidity_2m': 'mean',\n",
        "        'Temperature_10m': 'mean',\n",
        "        'SolarRadiation_W/m2': 'mean',\n",
        "        'SolarRadiation_kW/m2': 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    return aggregated_data[features]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create a dictionary to map sampling intervals to data frames."
      ],
      "metadata": {
        "id": "yhxzvZi-SGec"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdUura8zhqhI",
        "outputId": "d8b5ae89-cf73-4118-82af-30ae50d970f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'tower13': 'tower13_data_processed_1to9days.csv', 'tower4': 'tower4_data_processed_1to9days.csv', 'tower18': 'tower18_data_processed_1to9days.csv', 'tower24': 'tower24_data_processed_1to9days.csv', 'tower31': 'tower31_data_processed_1to9days.csv', 'tower2': 'tower2_data_processed_1to9days.csv', 'tower12': 'tower12_data_processed_1to9days.csv', 'tower25': 'tower25_data_processed_1to9days.csv', 'tower14': 'tower14_data_processed_1to9days.csv', 'tower10': 'tower10_data_processed_1to9days.csv', 'tower30': 'tower30_data_processed_1to9days.csv', 'tower6': 'tower6_data_processed_1to9days.csv', 'tower8': 'tower8_data_processed_1to9days.csv', 'tower32': 'tower32_data_processed_1to9days.csv', 'tower5': 'tower5_data_processed_1to9days.csv', 'tower28': 'tower28_data_processed_1to9days.csv', 'tower7': 'tower7_data_processed_1to9days.csv', 'tower9': 'tower9_data_processed_1to9days.csv', 'tower26': 'tower26_data_processed_1to9days.csv', 'tower15': 'tower15_data_processed_1to9days.csv', 'tower11': 'tower11_data_processed_1to9days.csv', 'tower21': 'tower21_data_processed_1to9days.csv', 'tower3': 'tower3_data_processed_1to9days.csv', 'tower23': 'tower23_data_processed_1to9days.csv', 'tower27': 'tower27_data_processed_1to9days.csv'}\n",
            "Processing tower13 with 15min and datetime FifteenMinDateTime\n",
            "Processing tower4 with 15min and datetime FifteenMinDateTime\n",
            "Processing tower18 with 15min and datetime FifteenMinDateTime\n",
            "Processing tower24 with 15min and datetime FifteenMinDateTime\n",
            "Processing tower31 with 15min and datetime FifteenMinDateTime\n",
            "Processing tower2 with 15min and datetime FifteenMinDateTime\n",
            "Processing tower12 with 15min and datetime FifteenMinDateTime\n",
            "Processing tower25 with 15min and datetime FifteenMinDateTime\n",
            "Processing tower14 with 15min and datetime FifteenMinDateTime\n",
            "Processing tower10 with 15min and datetime FifteenMinDateTime\n",
            "Processing tower30 with 15min and datetime FifteenMinDateTime\n",
            "Processing tower6 with 15min and datetime FifteenMinDateTime\n",
            "Processing tower8 with 15min and datetime FifteenMinDateTime\n",
            "Processing tower32 with 15min and datetime FifteenMinDateTime\n",
            "Processing tower5 with 15min and datetime FifteenMinDateTime\n",
            "Processing tower28 with 15min and datetime FifteenMinDateTime\n",
            "Processing tower7 with 15min and datetime FifteenMinDateTime\n",
            "Processing tower9 with 15min and datetime FifteenMinDateTime\n",
            "Processing tower26 with 15min and datetime FifteenMinDateTime\n",
            "Processing tower15 with 15min and datetime FifteenMinDateTime\n",
            "Processing tower11 with 15min and datetime FifteenMinDateTime\n",
            "Processing tower21 with 15min and datetime FifteenMinDateTime\n",
            "Processing tower3 with 15min and datetime FifteenMinDateTime\n",
            "Processing tower23 with 15min and datetime FifteenMinDateTime\n",
            "Processing tower27 with 15min and datetime FifteenMinDateTime\n",
            "Processing tower13 with 45min and datetime FortyFiveMinDateTime\n",
            "Processing tower4 with 45min and datetime FortyFiveMinDateTime\n",
            "Processing tower18 with 45min and datetime FortyFiveMinDateTime\n",
            "Processing tower24 with 45min and datetime FortyFiveMinDateTime\n",
            "Processing tower31 with 45min and datetime FortyFiveMinDateTime\n",
            "Processing tower2 with 45min and datetime FortyFiveMinDateTime\n",
            "Processing tower12 with 45min and datetime FortyFiveMinDateTime\n",
            "Processing tower25 with 45min and datetime FortyFiveMinDateTime\n",
            "Processing tower14 with 45min and datetime FortyFiveMinDateTime\n",
            "Processing tower10 with 45min and datetime FortyFiveMinDateTime\n",
            "Processing tower30 with 45min and datetime FortyFiveMinDateTime\n",
            "Processing tower6 with 45min and datetime FortyFiveMinDateTime\n",
            "Processing tower8 with 45min and datetime FortyFiveMinDateTime\n",
            "Processing tower32 with 45min and datetime FortyFiveMinDateTime\n",
            "Processing tower5 with 45min and datetime FortyFiveMinDateTime\n",
            "Processing tower28 with 45min and datetime FortyFiveMinDateTime\n",
            "Processing tower7 with 45min and datetime FortyFiveMinDateTime\n",
            "Processing tower9 with 45min and datetime FortyFiveMinDateTime\n",
            "Processing tower26 with 45min and datetime FortyFiveMinDateTime\n",
            "Processing tower15 with 45min and datetime FortyFiveMinDateTime\n",
            "Processing tower11 with 45min and datetime FortyFiveMinDateTime\n",
            "Processing tower21 with 45min and datetime FortyFiveMinDateTime\n",
            "Processing tower3 with 45min and datetime FortyFiveMinDateTime\n",
            "Processing tower23 with 45min and datetime FortyFiveMinDateTime\n",
            "Processing tower27 with 45min and datetime FortyFiveMinDateTime\n",
            "Processing tower13 with 5min and datetime FiveMinDateTime\n",
            "Processing tower4 with 5min and datetime FiveMinDateTime\n",
            "Processing tower18 with 5min and datetime FiveMinDateTime\n",
            "Processing tower24 with 5min and datetime FiveMinDateTime\n",
            "Processing tower31 with 5min and datetime FiveMinDateTime\n",
            "Processing tower2 with 5min and datetime FiveMinDateTime\n",
            "Processing tower12 with 5min and datetime FiveMinDateTime\n",
            "Processing tower25 with 5min and datetime FiveMinDateTime\n",
            "Processing tower14 with 5min and datetime FiveMinDateTime\n",
            "Processing tower10 with 5min and datetime FiveMinDateTime\n",
            "Processing tower30 with 5min and datetime FiveMinDateTime\n",
            "Processing tower6 with 5min and datetime FiveMinDateTime\n",
            "Processing tower8 with 5min and datetime FiveMinDateTime\n",
            "Processing tower32 with 5min and datetime FiveMinDateTime\n",
            "Processing tower5 with 5min and datetime FiveMinDateTime\n",
            "Processing tower28 with 5min and datetime FiveMinDateTime\n",
            "Processing tower7 with 5min and datetime FiveMinDateTime\n",
            "Processing tower9 with 5min and datetime FiveMinDateTime\n",
            "Processing tower26 with 5min and datetime FiveMinDateTime\n",
            "Processing tower15 with 5min and datetime FiveMinDateTime\n",
            "Processing tower11 with 5min and datetime FiveMinDateTime\n",
            "Processing tower21 with 5min and datetime FiveMinDateTime\n",
            "Processing tower3 with 5min and datetime FiveMinDateTime\n",
            "Processing tower23 with 5min and datetime FiveMinDateTime\n",
            "Processing tower27 with 5min and datetime FiveMinDateTime\n",
            "Processing tower13 with H and datetime HourlyDateTime\n",
            "Processing tower4 with H and datetime HourlyDateTime\n",
            "Processing tower18 with H and datetime HourlyDateTime\n",
            "Processing tower24 with H and datetime HourlyDateTime\n",
            "Processing tower31 with H and datetime HourlyDateTime\n",
            "Processing tower2 with H and datetime HourlyDateTime\n",
            "Processing tower12 with H and datetime HourlyDateTime\n",
            "Processing tower25 with H and datetime HourlyDateTime\n",
            "Processing tower14 with H and datetime HourlyDateTime\n",
            "Processing tower10 with H and datetime HourlyDateTime\n",
            "Processing tower30 with H and datetime HourlyDateTime\n",
            "Processing tower6 with H and datetime HourlyDateTime\n",
            "Processing tower8 with H and datetime HourlyDateTime\n",
            "Processing tower32 with H and datetime HourlyDateTime\n",
            "Processing tower5 with H and datetime HourlyDateTime\n",
            "Processing tower28 with H and datetime HourlyDateTime\n",
            "Processing tower7 with H and datetime HourlyDateTime\n",
            "Processing tower9 with H and datetime HourlyDateTime\n",
            "Processing tower26 with H and datetime HourlyDateTime\n",
            "Processing tower15 with H and datetime HourlyDateTime\n",
            "Processing tower11 with H and datetime HourlyDateTime\n",
            "Processing tower21 with H and datetime HourlyDateTime\n",
            "Processing tower3 with H and datetime HourlyDateTime\n",
            "Processing tower23 with H and datetime HourlyDateTime\n",
            "Processing tower27 with H and datetime HourlyDateTime\n",
            "Processing tower13 with 30min and datetime HalfHourlyDateTime\n",
            "Processing tower4 with 30min and datetime HalfHourlyDateTime\n",
            "Processing tower18 with 30min and datetime HalfHourlyDateTime\n",
            "Processing tower24 with 30min and datetime HalfHourlyDateTime\n",
            "Processing tower31 with 30min and datetime HalfHourlyDateTime\n",
            "Processing tower2 with 30min and datetime HalfHourlyDateTime\n",
            "Processing tower12 with 30min and datetime HalfHourlyDateTime\n",
            "Processing tower25 with 30min and datetime HalfHourlyDateTime\n",
            "Processing tower14 with 30min and datetime HalfHourlyDateTime\n",
            "Processing tower10 with 30min and datetime HalfHourlyDateTime\n",
            "Processing tower30 with 30min and datetime HalfHourlyDateTime\n",
            "Processing tower6 with 30min and datetime HalfHourlyDateTime\n",
            "Processing tower8 with 30min and datetime HalfHourlyDateTime\n",
            "Processing tower32 with 30min and datetime HalfHourlyDateTime\n",
            "Processing tower5 with 30min and datetime HalfHourlyDateTime\n",
            "Processing tower28 with 30min and datetime HalfHourlyDateTime\n",
            "Processing tower7 with 30min and datetime HalfHourlyDateTime\n",
            "Processing tower9 with 30min and datetime HalfHourlyDateTime\n",
            "Processing tower26 with 30min and datetime HalfHourlyDateTime\n",
            "Processing tower15 with 30min and datetime HalfHourlyDateTime\n",
            "Processing tower11 with 30min and datetime HalfHourlyDateTime\n",
            "Processing tower21 with 30min and datetime HalfHourlyDateTime\n",
            "Processing tower3 with 30min and datetime HalfHourlyDateTime\n",
            "Processing tower23 with 30min and datetime HalfHourlyDateTime\n",
            "Processing tower27 with 30min and datetime HalfHourlyDateTime\n"
          ]
        }
      ],
      "source": [
        "towerToCSV = dict()\n",
        "root = 'IoBT All Tower Data/'\n",
        "idx = 0\n",
        "for tower, data in towerMap.items():\n",
        "    towerToCSV[tower] = allFiles[idx]\n",
        "    idx +=1\n",
        "print(towerToCSV)\n",
        "intervals = {\n",
        "    \"15min\": \"FifteenMinDateTime\",\n",
        "    \"45min\": \"FortyFiveMinDateTime\",\n",
        "    \"5min\": \"FiveMinDateTime\",\n",
        "    \"H\": \"HourlyDateTime\",\n",
        "    \"30min\": \"HalfHourlyDateTime\"\n",
        "}\n",
        "\n",
        "# Create a dictionary to hold the list for each interval.\n",
        "tower_lists = {}\n",
        "\n",
        "for interval, datetime_col_name in intervals.items():\n",
        "    # Create an empty list for the current interval.\n",
        "    tower_list = []\n",
        "\n",
        "    for tower, data in towerMap.items():\n",
        "        # Process data using the provided function.\n",
        "        print(f\"Processing {tower} with {interval} and datetime {datetime_col_name}\")\n",
        "        tower_data = group_data_by_interval(pd.read_csv(root + towerToCSV[tower]), round_interval=interval, datetime_col_name=datetime_col_name)\n",
        "        tower_list.append(tower_data)\n",
        "\n",
        "    # Save the resulting list in the dictionary.\n",
        "    tower_lists[interval] = tower_list\n",
        "# print(tower_lists['15min'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toW88sqTSoXd",
        "outputId": "24d6bc72-c38c-4ae5-cd9c-b1ae1e60289c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(2604, 5), (2604, 5), (2604, 5), (2604, 5), (2604, 5), (2604, 5), (2604, 5), (2604, 5), (2604, 5), (2604, 5), (2604, 5), (2604, 5), (2604, 5), (2604, 5), (2604, 5), (2604, 5), (2604, 5), (2604, 5), (2604, 5), (2604, 5), (2604, 5), (2604, 5), (2604, 5), (2604, 5), (2604, 5)]\n"
          ]
        }
      ],
      "source": [
        "# find the shortest length\n",
        "min_len = min(arr.shape[0] for arr in tower_lists['5min'])\n",
        "\n",
        "# truncate every array/DataFrame to that length\n",
        "tower_lists['5min'] = [\n",
        "    arr.iloc[:min_len] if hasattr(arr, 'iloc') else arr[:min_len]\n",
        "    for arr in tower_lists['5min']\n",
        "]\n",
        "\n",
        "# check\n",
        "print([arr.shape for arr in tower_lists['5min']])\n",
        "# now they should all be (2604, 4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTDpOVrPxtRj",
        "outputId": "77b1fb63-99e3-4ebf-a4de-4369333ea530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_values(['tower13_data_processed_1to9days.csv', 'tower4_data_processed_1to9days.csv', 'tower18_data_processed_1to9days.csv', 'tower24_data_processed_1to9days.csv', 'tower31_data_processed_1to9days.csv', 'tower2_data_processed_1to9days.csv', 'tower12_data_processed_1to9days.csv', 'tower25_data_processed_1to9days.csv', 'tower14_data_processed_1to9days.csv', 'tower10_data_processed_1to9days.csv', 'tower30_data_processed_1to9days.csv', 'tower6_data_processed_1to9days.csv', 'tower8_data_processed_1to9days.csv', 'tower32_data_processed_1to9days.csv', 'tower5_data_processed_1to9days.csv', 'tower28_data_processed_1to9days.csv', 'tower7_data_processed_1to9days.csv', 'tower9_data_processed_1to9days.csv', 'tower26_data_processed_1to9days.csv', 'tower15_data_processed_1to9days.csv', 'tower11_data_processed_1to9days.csv', 'tower21_data_processed_1to9days.csv', 'tower3_data_processed_1to9days.csv', 'tower23_data_processed_1to9days.csv', 'tower27_data_processed_1to9days.csv'])\n",
            "['MAC000013', 'MAC000004', 'MAC000018', 'MAC000024', 'MAC000031', 'MAC000002', 'MAC000012', 'MAC000025', 'MAC000014', 'MAC000010', 'MAC000030', 'MAC000006', 'MAC000008', 'MAC000032', 'MAC000005', 'MAC000028', 'MAC000007', 'MAC000009', 'MAC000026', 'MAC000015', 'MAC000011', 'MAC000021', 'MAC000003', 'MAC000023', 'MAC000027']\n",
            "Nodes: 8\n",
            "Time: 15min Processing FifteenMinDateTime\n",
            "Time: 45min Processing FortyFiveMinDateTime\n",
            "Time: 5min Processing FiveMinDateTime\n",
            "Time: H Processing HourlyDateTime\n",
            "Time: 30min Processing HalfHourlyDateTime\n",
            "Nodes: 16\n",
            "Time: 15min Processing FifteenMinDateTime\n",
            "Time: 45min Processing FortyFiveMinDateTime\n",
            "Time: 5min Processing FiveMinDateTime\n",
            "Time: H Processing HourlyDateTime\n",
            "Time: 30min Processing HalfHourlyDateTime\n",
            "Nodes: 25\n",
            "Time: 15min Processing FifteenMinDateTime\n",
            "Time: 45min Processing FortyFiveMinDateTime\n",
            "Time: 5min Processing FiveMinDateTime\n",
            "(2604, 5)\n",
            "Time: H Processing HourlyDateTime\n",
            "Time: 30min Processing HalfHourlyDateTime\n"
          ]
        }
      ],
      "source": [
        "print(towerToCSV.values())\n",
        "tower_labels = [\n",
        "    f\"MAC{num:06d}\"\n",
        "    for num in [int(key[5:]) for key in towerMap.keys()]\n",
        "]\n",
        "print(tower_labels)\n",
        "\n",
        "def csv(k):\n",
        "  print(f\"Nodes: {k}\")\n",
        "  for time, tower_list in tower_lists.items():\n",
        "    print(f\"Time: {time} Processing {intervals[time]}\")\n",
        "    df = pd.DataFrame()\n",
        "    df['ds'] = tower_list[0][intervals[time]]\n",
        "    row,col = tower_list[0].shape\n",
        "    for tower, label in zip(tower_list[:k], tower_labels[:k]):\n",
        "        if tower.shape[0] != row or tower.shape[1] != col:\n",
        "          print(tower.shape)\n",
        "        df[label] = tower['Temperature_2m']\n",
        "    df.set_index('ds', inplace=True)\n",
        "    if time == \"H\":\n",
        "      df.to_csv(f'temperature_data_60min_{k}.csv')\n",
        "    else:\n",
        "      df.to_csv(f'temperature_data_{time}_{k}.csv')\n",
        "csv(8)\n",
        "csv(16)\n",
        "csv(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JuFpMBrhRjI",
        "outputId": "89cfd126-6e0e-4c10-c9d9-a4b8f94ff8ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Benchmark_STGNN_for_STLF.zip\n",
            "   creating: Benchmark_STGNN_for_STLF/\n",
            "   creating: Benchmark_STGNN_for_STLF/.git/\n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/config  \n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/description  \n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/HEAD  \n",
            "   creating: Benchmark_STGNN_for_STLF/.git/hooks/\n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/hooks/applypatch-msg.sample  \n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/hooks/commit-msg.sample  \n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/hooks/fsmonitor-watchman.sample  \n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/hooks/post-update.sample  \n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/hooks/pre-applypatch.sample  \n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/hooks/pre-commit.sample  \n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/hooks/pre-merge-commit.sample  \n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/hooks/pre-push.sample  \n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/hooks/pre-rebase.sample  \n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/hooks/pre-receive.sample  \n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/hooks/prepare-commit-msg.sample  \n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/hooks/push-to-checkout.sample  \n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/hooks/sendemail-validate.sample  \n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/hooks/update.sample  \n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/index  \n",
            "   creating: Benchmark_STGNN_for_STLF/.git/info/\n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/info/exclude  \n",
            "   creating: Benchmark_STGNN_for_STLF/.git/logs/\n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/logs/HEAD  \n",
            "   creating: Benchmark_STGNN_for_STLF/.git/logs/refs/\n",
            "   creating: Benchmark_STGNN_for_STLF/.git/logs/refs/heads/\n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/logs/refs/heads/main  \n",
            "   creating: Benchmark_STGNN_for_STLF/.git/logs/refs/remotes/\n",
            "   creating: Benchmark_STGNN_for_STLF/.git/logs/refs/remotes/origin/\n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/logs/refs/remotes/origin/HEAD  \n",
            "   creating: Benchmark_STGNN_for_STLF/.git/objects/\n",
            "   creating: Benchmark_STGNN_for_STLF/.git/objects/info/\n",
            "   creating: Benchmark_STGNN_for_STLF/.git/objects/pack/\n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/objects/pack/pack-a82a1698d64e119095c83e5aa690435e6e235a2a.idx  \n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/objects/pack/pack-a82a1698d64e119095c83e5aa690435e6e235a2a.pack  \n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/objects/pack/pack-a82a1698d64e119095c83e5aa690435e6e235a2a.rev  \n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/packed-refs  \n",
            "   creating: Benchmark_STGNN_for_STLF/.git/refs/\n",
            "   creating: Benchmark_STGNN_for_STLF/.git/refs/heads/\n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/refs/heads/main  \n",
            "   creating: Benchmark_STGNN_for_STLF/.git/refs/remotes/\n",
            "   creating: Benchmark_STGNN_for_STLF/.git/refs/remotes/origin/\n",
            "  inflating: Benchmark_STGNN_for_STLF/.git/refs/remotes/origin/HEAD  \n",
            "   creating: Benchmark_STGNN_for_STLF/.git/refs/tags/\n",
            "  inflating: Benchmark_STGNN_for_STLF/.gitignore  \n",
            "  inflating: Benchmark_STGNN_for_STLF/config.yaml  \n",
            "   creating: Benchmark_STGNN_for_STLF/custom_models/\n",
            "  inflating: Benchmark_STGNN_for_STLF/custom_models/baseline.py  \n",
            "  inflating: Benchmark_STGNN_for_STLF/custom_models/bipartite.py  \n",
            "  inflating: Benchmark_STGNN_for_STLF/custom_models/gcgru.py  \n",
            "  inflating: Benchmark_STGNN_for_STLF/custom_models/gclstm.py  \n",
            "  inflating: Benchmark_STGNN_for_STLF/custom_models/gts.py  \n",
            "  inflating: Benchmark_STGNN_for_STLF/custom_models/stegnn.py  \n",
            "  inflating: Benchmark_STGNN_for_STLF/custom_models/tgcn.py  \n",
            "  inflating: Benchmark_STGNN_for_STLF/custom_models/TimeThenSpace.py  \n",
            "  inflating: Benchmark_STGNN_for_STLF/custom_models/__init__.py  \n",
            "   creating: Benchmark_STGNN_for_STLF/data/\n",
            "  inflating: Benchmark_STGNN_for_STLF/data/DataLCL_228houses_with_timeslot_temperature.csv  \n",
            "   creating: Benchmark_STGNN_for_STLF/graph_generation/\n",
            "   creating: Benchmark_STGNN_for_STLF/graph_generation/cache/\n",
            "   creating: Benchmark_STGNN_for_STLF/graph_generation/cache/228/\n",
            "  inflating: Benchmark_STGNN_for_STLF/graph_generation/cache/228/correntropy_adjacency_matrix.npy  \n",
            "  inflating: Benchmark_STGNN_for_STLF/graph_generation/cache/228/dtw_adjacency_matrix.npy  \n",
            "  inflating: Benchmark_STGNN_for_STLF/graph_generation/cache/228/euclidean_adjacency_matrix.npy  \n",
            "  inflating: Benchmark_STGNN_for_STLF/graph_generation/cache/228/pearson_adjacency_matrix.npy  \n",
            "  inflating: Benchmark_STGNN_for_STLF/graph_generation/cache/228/transfer_entropy_adjacency_matrix.npy  \n",
            "  inflating: Benchmark_STGNN_for_STLF/graph_generation/cache/correntropy_adjacency_matrix.npy  \n",
            "  inflating: Benchmark_STGNN_for_STLF/graph_generation/cache/dtw_adjacency_matrix.npy  \n",
            "  inflating: Benchmark_STGNN_for_STLF/graph_generation/cache/euclidean_adjacency_matrix.npy  \n",
            "  inflating: Benchmark_STGNN_for_STLF/graph_generation/cache/pearson_adjacency_matrix.npy  \n",
            "  inflating: Benchmark_STGNN_for_STLF/graph_generation/fixed_graph.py  \n",
            "  inflating: Benchmark_STGNN_for_STLF/graph_generation/__init__.py  \n",
            "  inflating: Benchmark_STGNN_for_STLF/hyperparameter_tuning.py  \n",
            "   creating: Benchmark_STGNN_for_STLF/lightning_logs/\n",
            "   creating: Benchmark_STGNN_for_STLF/lightning_logs/version_0/\n",
            "   creating: Benchmark_STGNN_for_STLF/lightning_logs/version_0/checkpoints/\n",
            "  inflating: Benchmark_STGNN_for_STLF/lightning_logs/version_0/checkpoints/epoch=0-step=100.ckpt  \n",
            "  inflating: Benchmark_STGNN_for_STLF/lightning_logs/version_0/events.out.tfevents.1721402428.UNIC02D563YMD6R.9480.0  \n",
            "  inflating: Benchmark_STGNN_for_STLF/lightning_logs/version_0/hparams.yaml  \n",
            "  inflating: Benchmark_STGNN_for_STLF/README.md  \n",
            "  inflating: Benchmark_STGNN_for_STLF/requirements.txt  \n",
            "  inflating: Benchmark_STGNN_for_STLF/SpatioTemporal_TS_with_Graph.py  \n",
            "   creating: Benchmark_STGNN_for_STLF/tools/\n",
            "  inflating: Benchmark_STGNN_for_STLF/tools/evaluate_test_results.py  \n",
            "  inflating: Benchmark_STGNN_for_STLF/tools/plot_forecast.py  \n",
            "  inflating: Benchmark_STGNN_for_STLF/tools/__init__.py  \n"
          ]
        }
      ],
      "source": [
        "!unzip \"Benchmark_STGNN_for_STLF.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzSFbp6CwOwn",
        "outputId": "a7a05981-4eb2-4fd3-8379-c713e5125c03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting h5py\n",
            "  Downloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from h5py) (1.26.4)\n",
            "Downloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m195.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h5py\n",
            "Successfully installed h5py-3.13.0\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (4.13.2)\n",
            "Collecting wheel\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wheel\n",
            "Successfully installed wheel-0.45.1\n"
          ]
        }
      ],
      "source": [
        "!pip install h5py\n",
        "!pip install typing-extensions\n",
        "!pip install wheel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install dependencies for Repo"
      ],
      "metadata": {
        "id": "eM71ByUUTMLi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xawB_RAyB7v8",
        "outputId": "f799e3ec-c2b7-4349-e0bb-b4feac9832c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.4.1\n",
            "Uninstalling torch-2.4.1:\n",
            "  Successfully uninstalled torch-2.4.1\n",
            "\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-sparse as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping pytorch-lightning as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping lightning as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-spatiotemporal as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: pandas 1.5.3\n",
            "Uninstalling pandas-1.5.3:\n",
            "  Successfully uninstalled pandas-1.5.3\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu117\n",
            "Collecting torch==2.0.0+cu117\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torch-2.0.0%2Bcu117-cp311-cp311-linux_x86_64.whl (1843.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.15.1+cu117\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torchvision-0.15.1%2Bcu117-cp311-cp311-linux_x86_64.whl (6.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.0.1+cu117\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torchaudio-2.0.1%2Bcu117-cp311-cp311-linux_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m115.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0+cu117) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0+cu117) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0+cu117) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0+cu117) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0+cu117) (3.1.6)\n",
            "Collecting triton==2.0.0 (from torch==2.0.0+cu117)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.1+cu117) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.1+cu117) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.1+cu117) (10.4.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.0+cu117) (3.31.6)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.0+cu117)\n",
            "  Downloading https://download.pytorch.org/whl/lit-15.0.7.tar.gz (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.0+cu117) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.1+cu117) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.1+cu117) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.1+cu117) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.1+cu117) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.0+cu117) (1.3.0)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-15.0.7-py3-none-any.whl size=89991 sha256=e95b99f39932f6a29444e203b0c78650be2da3c1ad6fdc78318a81a0bbe76c6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/5d/45/34fe9945d5e45e261134e72284395be36c2d4828af38e2b0fe\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, triton, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.0.0\n",
            "    Uninstalling triton-3.0.0:\n",
            "      Successfully uninstalled triton-3.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandasai 2.4.2 requires pandas==1.5.3, which is not installed.\n",
            "timm 1.0.15 requires huggingface_hub, which is not installed.\n",
            "peft 0.15.2 requires huggingface_hub>=0.25.0, which is not installed.\n",
            "peft 0.15.2 requires transformers, which is not installed.\n",
            "accelerate 1.6.0 requires huggingface-hub>=0.21.0, which is not installed.\n",
            "fastai 2.7.19 requires pandas, which is not installed.\n",
            "sentence-transformers 4.1.0 requires huggingface-hub>=0.20.0, which is not installed.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, which is not installed.\n",
            "pandasai 2.4.2 requires torch==2.4.1; sys_platform != \"darwin\", but you have torch 2.0.0+cu117 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-15.0.7 torch-2.0.0+cu117 torchaudio-2.0.1+cu117 torchvision-0.15.1+cu117 triton-2.0.0\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cu117.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu117/torch_scatter-2.1.2%2Bpt20cu117-cp311-cp311-linux_x86_64.whl (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu117/torch_sparse-0.6.18%2Bpt20cu117-cp311-cp311-linux_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter, torch-sparse, torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1 torch-scatter-2.1.2+pt20cu117 torch-sparse-0.6.18+pt20cu117\n",
            "Collecting pytorch-lightning==2.0.9\n",
            "  Downloading pytorch_lightning-2.0.9-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting torch-spatiotemporal==0.9.5\n",
            "  Downloading torch_spatiotemporal-0.9.5-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pandas==2.2.2\n",
            "  Using cached pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting scipy==1.11.4\n",
            "  Downloading scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dtw-python==1.5.3\n",
            "  Downloading dtw_python-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops==0.8.0\n",
            "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pydantic==2.9.2\n",
            "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets==14.0.0\n",
            "  Downloading websockets-14.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting fsspec==2024.12.0\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting gcsfs==2024.12.0\n",
            "  Downloading gcsfs-2024.12.0-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting matplotlib==3.9.2\n",
            "  Downloading matplotlib-3.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting pyinform==0.2.0\n",
            "  Downloading pyinform-0.2.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting optuna==3.1.0\n",
            "  Downloading optuna-3.1.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==2.0.9) (2.0.0+cu117)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==2.0.9) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==2.0.9) (6.0.2)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning==2.0.9)\n",
            "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==2.0.9) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==2.0.9) (4.13.2)\n",
            "Collecting lightning-utilities>=0.7.0 (from pytorch-lightning==2.0.9)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from torch-spatiotemporal==0.9.5) (1.6.1)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.11/dist-packages (from torch-spatiotemporal==0.9.5) (3.10.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic==2.9.2) (0.7.0)\n",
            "Collecting pydantic-core==2.23.4 (from pydantic==2.9.2)\n",
            "  Downloading pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from gcsfs==2024.12.0) (3.11.15)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs==2024.12.0) (4.4.2)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs==2024.12.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (from gcsfs==2024.12.0) (1.2.2)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (from gcsfs==2024.12.0) (2.19.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from gcsfs==2024.12.0) (2.32.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.2) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.2) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.2) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.2) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.2) (3.2.3)\n",
            "Collecting alembic>=1.5.0 (from optuna==3.1.0)\n",
            "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting cmaes>=0.9.1 (from optuna==3.1.0)\n",
            "  Downloading cmaes-0.11.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting colorlog (from optuna==3.1.0)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from optuna==3.1.0) (2.0.40)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.12.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.12.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.12.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.12.0) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.12.0) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.12.0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs==2024.12.0) (1.20.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna==3.1.0) (1.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs==2024.12.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs==2024.12.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs==2024.12.0) (4.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.7.0->pytorch-lightning==2.0.9) (75.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.3.0->optuna==3.1.0) (3.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->pytorch-lightning==2.0.9) (3.18.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->pytorch-lightning==2.0.9) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->pytorch-lightning==2.0.9) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->pytorch-lightning==2.0.9) (3.1.6)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->pytorch-lightning==2.0.9) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning==2.0.9) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning==2.0.9) (15.0.7)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib->gcsfs==2024.12.0) (2.0.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs==2024.12.0) (2.24.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs==2024.12.0) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs==2024.12.0) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs==2024.12.0) (1.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs==2024.12.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs==2024.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs==2024.12.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs==2024.12.0) (2025.4.26)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->torch-spatiotemporal==0.9.5) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->torch-spatiotemporal==0.9.5) (3.6.0)\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.11/dist-packages (from tables->torch-spatiotemporal==0.9.5) (2.10.2)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from tables->torch-spatiotemporal==0.9.5) (9.0.0)\n",
            "Requirement already satisfied: blosc2>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from tables->torch-spatiotemporal==0.9.5) (3.3.2)\n",
            "Requirement already satisfied: ndindex in /usr/local/lib/python3.11/dist-packages (from blosc2>=2.3.0->tables->torch-spatiotemporal==0.9.5) (1.9.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from blosc2>=2.3.0->tables->torch-spatiotemporal==0.9.5) (1.1.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from blosc2>=2.3.0->tables->torch-spatiotemporal==0.9.5) (4.3.8)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs==2024.12.0) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs==2024.12.0) (5.29.4)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs==2024.12.0) (1.26.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs==2024.12.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==2024.12.0) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->pytorch-lightning==2.0.9) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.11.0->pytorch-lightning==2.0.9) (1.3.0)\n",
            "Downloading pytorch_lightning-2.0.9-py3-none-any.whl (727 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.7/727.7 kB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_spatiotemporal-0.9.5-py3-none-any.whl (253 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "Downloading scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dtw_python-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (801 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.7/801.7 kB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-14.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gcsfs-2024.12.0-py2.py3-none-any.whl (35 kB)\n",
            "Downloading matplotlib-3.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m135.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyinform-0.2.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.2/131.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna-3.1.0-py3-none-any.whl (365 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.3/365.3 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cmaes-0.11.1-py3-none-any.whl (35 kB)\n",
            "Downloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: websockets, scipy, pyinform, pydantic-core, lightning-utilities, fsspec, einops, colorlog, cmaes, pydantic, pandas, matplotlib, dtw-python, alembic, optuna, gcsfs, torchmetrics, pytorch-lightning, torch-spatiotemporal\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.33.2\n",
            "    Uninstalling pydantic_core-2.33.2:\n",
            "      Successfully uninstalled pydantic_core-2.33.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.4\n",
            "    Uninstalling pydantic-2.11.4:\n",
            "      Successfully uninstalled pydantic-2.11.4\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.3\n",
            "    Uninstalling matplotlib-3.10.3:\n",
            "      Successfully uninstalled matplotlib-3.10.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires jax>=0.1.72, which is not installed.\n",
            "dopamine-rl 4.1.2 requires jaxlib>=0.1.51, which is not installed.\n",
            "datasets 2.14.4 requires huggingface-hub<1.0.0,>=0.14.0, which is not installed.\n",
            "datasets 2.14.4 requires multiprocess, which is not installed.\n",
            "tensorflow-decision-forests 1.11.0 requires wheel, which is not installed.\n",
            "sentence-transformers 4.1.0 requires huggingface-hub>=0.20.0, which is not installed.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, which is not installed.\n",
            "pandasai 2.4.2 requires pandas==1.5.3, but you have pandas 2.2.2 which is incompatible.\n",
            "pandasai 2.4.2 requires torch==2.4.1; sys_platform != \"darwin\", but you have torch 2.0.0+cu117 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed alembic-1.15.2 cmaes-0.11.1 colorlog-6.9.0 dtw-python-1.5.3 einops-0.8.0 fsspec-2024.12.0 gcsfs-2024.12.0 lightning-utilities-0.14.3 matplotlib-3.9.2 optuna-3.1.0 pandas-2.2.2 pydantic-2.9.2 pydantic-core-2.23.4 pyinform-0.2.0 pytorch-lightning-2.0.9 scipy-1.11.4 torch-spatiotemporal-0.9.5 torchmetrics-1.7.1 websockets-14.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1. Full clean uninstall\n",
        "!pip uninstall -y torch torchvision torchaudio torch-scatter torch-sparse torch-geometric pytorch-lightning lightning torch-spatiotemporal pandas\n",
        "\n",
        "# 2. Install PyTorch 2.0.0 with CUDA 11.7 (matches Colab's default environment)\n",
        "!pip install torch==2.0.0+cu117 torchvision==0.15.1+cu117 torchaudio==2.0.1+cu117 --index-url https://download.pytorch.org/whl/cu117\n",
        "\n",
        "# 3. Install PyG dependencies FOR TORCH 2.0.0\n",
        "!pip install torch-scatter torch-sparse torch-geometric -f https://data.pyg.org/whl/torch-2.0.0+cu117.html\n",
        "\n",
        "# 4. Install remaining packages WITH VERSION PINS\n",
        "!pip install \\\n",
        "  pytorch-lightning==2.0.9 \\\n",
        "  torch-spatiotemporal==0.9.5 \\\n",
        "  pandas==2.2.2 \\\n",
        "  numpy==1.26.4 \\\n",
        "  scipy==1.11.4 \\\n",
        "  dtw-python==1.5.3 \\\n",
        "  einops==0.8.0 \\\n",
        "  pydantic==2.9.2 \\\n",
        "  websockets==14.0.0 \\\n",
        "  fsspec==2024.12.0 \\\n",
        "  gcsfs==2024.12.0 \\\n",
        "  matplotlib==3.9.2 \\\n",
        "  pyinform==0.2.0  \\\n",
        "  optuna==3.1.0\n",
        "\n",
        "# 5. Create directories\n",
        "!mkdir -p Benchmark_STGNN_for_STLF/data \\\n",
        "         Benchmark_STGNN_for_STLF/visualization/adj_heatmap \\\n",
        "         Benchmark_STGNN_for_STLF/visualization/forecast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHMrzVBCAF9L",
        "outputId": "57a5b575-80bc-478a-fb2f-d4d1f31d7290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Python 3.11.12\n",
            "7.34.0\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p Benchmark_STGNN_for_STLF/data /content/Benchmark_STGNN_for_STLF/visualization/adj_heatmap /content/Benchmark_STGNN_for_STLF/visualization/forecast"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the experiments for GCN Models"
      ],
      "metadata": {
        "id": "NHcDRktDUPQ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNIB5ughWqt5",
        "outputId": "0cdf6f01-157c-4be5-ec57-5db7a75b7082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.0+cu117\n",
            "Window size is 96 and horizon is 48 for sampling rate of 5 mins\n",
            "tsl version  : 0.9.5\n",
            "torch version: 2.0.0+cu117\n",
            "The chosen model is grugcn_model\n",
            "Shape of data: (2605, 8)\n",
            "8\n",
            "Before downsampling: Shape of df matrix: (2605, 8)\n",
            "After downsampling: (2605, 8)\n",
            "args.redundancy: 0\n",
            "Requested: 0% Redundancy | Actual: 0.0% | Threshold: 2.000\n",
            "Calculated and saved adjacency matrix to /content/Benchmark_STGNN_for_STLF/graph_generation/cache/pearson_adjacency_matrix.npy\n",
            "Count of non-zero elements in adjacency matrix: 0\n",
            "Shape of adj matrix is (8, 8)\n",
            "Print heatmap of adjacency matrix\n",
            "Figure(1000x800)\n",
            "Length of train, val and test splits are: 21, 9, 9\n",
            "Metadata of pearson is: {'n_channels': 1, 'horizon': 48, 'n_nodes': 8, 'window': 96}\n",
            "\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
            "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "2025-05-15 04:59:20.940047: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-15 04:59:20.959371: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747285160.980630   32998 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747285160.987039   32998 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-15 04:59:21.008037: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name          | Type             | Params\n",
            "---------------------------------------------------\n",
            "0 | loss_fn       | MaskedMAE        | 0     \n",
            "1 | train_metrics | MetricCollection | 0     \n",
            "2 | val_metrics   | MetricCollection | 0     \n",
            "3 | test_metrics  | MetricCollection | 0     \n",
            "4 | model         | GRUGCNModel      | 28.5 K\n",
            "---------------------------------------------------\n",
            "28.5 K    Trainable params\n",
            "0         Non-trainable params\n",
            "28.5 K    Total params\n",
            "0.114     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Metric val_loss improved. New best score: 2.454\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Monitored metric val_loss did not improve in the last 5 records. Best score: 2.454. Signaling Trainer to stop.\n",
            "Predictor\n",
            "Restoring states from the checkpoint path at checkpoint/GRUGCNModel/best-4-2.45.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from the checkpoint at checkpoint/GRUGCNModel/best-4-2.45.ckpt\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m Runningstage.validating \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                           \u001b[0m┃\n",
            "┃\u001b[1m \u001b[0m\u001b[1m         metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.4536643028259277    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         val_mae         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.4536643028259277    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m      val_mae_at_30      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.6702147722244263    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m        val_mape         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.3452085554599762    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Restoring states from the checkpoint path at checkpoint/GRUGCNModel/best-4-2.45.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from the checkpoint at checkpoint/GRUGCNModel/best-4-2.45.ckpt\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m  Runningstage.testing   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                           \u001b[0m┃\n",
            "┃\u001b[1m \u001b[0m\u001b[1m         metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.6579976081848145    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m        test_mae         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.6579976081848145    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m     test_mae_at_30      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.980937123298645    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m        test_mape        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.24710431694984436   \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Restoring states from the checkpoint path at checkpoint/GRUGCNModel/best-4-2.45.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from the checkpoint at checkpoint/GRUGCNModel/best-4-2.45.ckpt\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n",
            "Figure(1000x500)\n",
            "MAE at validation dataset is: 2.4536643028259277\n",
            "\n",
            "real\t0m20.908s\n",
            "user\t0m37.621s\n",
            "sys\t0m5.267s\n",
            "DONE: Iteration: 1/360\t GRUGCN\t Metric: pearson\tRedundancy: 0\tSampling: 5\tNodes: 8\n",
            "2.0.0+cu117\n",
            "Window size is 32 and horizon is 16 for sampling rate of 15 mins\n",
            "tsl version  : 0.9.5\n",
            "torch version: 2.0.0+cu117\n",
            "The chosen model is grugcn_model\n",
            "Shape of data: (869, 8)\n",
            "8\n",
            "Before downsampling: Shape of df matrix: (869, 8)\n",
            "After downsampling: (869, 8)\n",
            "args.redundancy: 0\n",
            "Requested: 0% Redundancy | Actual: 0.0% | Threshold: 2.000\n",
            "Calculated and saved adjacency matrix to /content/Benchmark_STGNN_for_STLF/graph_generation/cache/pearson_adjacency_matrix.npy\n",
            "Count of non-zero elements in adjacency matrix: 0\n",
            "Shape of adj matrix is (8, 8)\n",
            "Print heatmap of adjacency matrix\n",
            "Figure(1000x800)\n",
            "Length of train, val and test splits are: 7, 3, 3\n",
            "Metadata of pearson is: {'n_channels': 1, 'horizon': 16, 'n_nodes': 8, 'window': 32}\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Benchmark_STGNN_for_STLF/SpatioTemporal_TS_with_Graph.py\", line 551, in <module>\n",
            "    main()\n",
            "  File \"/content/Benchmark_STGNN_for_STLF/SpatioTemporal_TS_with_Graph.py\", line 450, in main\n",
            "    bipartite_model = BiPartiteSTGraphModel(\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Benchmark_STGNN_for_STLF/custom_models/bipartite.py\", line 55, in __init__\n",
            "    self.main_to_aux_edge_index = self.main_to_aux_edge_index.nonzero().t().contiguous()\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "DONE: Iteration: 2/360\t GRUGCN\t Metric: pearson\tRedundancy: 0\tSampling: 15\tNodes: 8\n",
            "^C\n",
            "DONE: Iteration: 3/360\t GRUGCN\t Metric: pearson\tRedundancy: 0\tSampling: 30\tNodes: 8\n",
            "^C\n",
            "^C\n",
            "DONE: Iteration: 4/360\t GRUGCN\t Metric: pearson\tRedundancy: 0\tSampling: 45\tNodes: 8\n",
            "^C\n",
            "DONE: Iteration: 5/360\t GRUGCN\t Metric: pearson\tRedundancy: 0\tSampling: 60\tNodes: 8\n",
            "^C\n",
            "DONE: Iteration: 6/360\t GRUGCN\t Metric: pearson\tRedundancy: 20\tSampling: 5\tNodes: 8\n"
          ]
        }
      ],
      "source": [
        "!rm -rf save_inference_result\n",
        "!mkdir -p save_inference_result\n",
        "nodes = [8,16,25]\n",
        "metrics = [\"pearson\", \"euclidean\"]\n",
        "models = {\"grugcn_model\":\"GRUGCN\", \"tgcn_model\": \"TGCN\", }\n",
        "redundancies = [0,20,40,60,80,100]\n",
        "samplings = [5,15,30,45,60]\n",
        "total = len(models) * len(nodes) * len(redundancies) * len(samplings) * len(metrics)\n",
        "count = 0\n",
        "for model,name in models.items():\n",
        "  for node in nodes:\n",
        "    for metric in metrics:\n",
        "      for redundancy in redundancies:\n",
        "        for sampling in samplings:\n",
        "          !rm -f /content/Benchmark_STGNN_for_STLF/graph_generation/cache/{metric}_adjacency_matrix.npy\n",
        "          !time python3 Benchmark_STGNN_for_STLF/SpatioTemporal_TS_with_Graph.py \\\n",
        "          {model} {name}_{redundancy}%_{sampling}mins_{metric}_{node}Nodes --method {metric} --window 8 --hidden_dimension 64 \\\n",
        "          --learning_rate 0.001 --batch_size 64 --horizon 4 --sampling {sampling} --redundancy {redundancy} --nodes {node}\n",
        "          !python Benchmark_STGNN_for_STLF/tools/evaluate_test_results.py save_inference_result/{name}_{redundancy}%_{sampling}mins_{metric}_{node}Nodes/{metric} > /dev/null 2>&1\n",
        "          count += 1\n",
        "          print(f\"DONE: Iteration: {count}/{total}\\t {name}\\t Metric: {metric}\\tRedundancy: {redundancy}\\tSampling: {sampling}\\tNodes: {node}\")\n",
        "!zip -r save_inference_result_GCN_Temperature_Models.zip save_inference_result\n",
        "from google.colab import files\n",
        "files.download('save_inference_result_GCN_Temperature_Models.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the experiments for Pure Temporal Models"
      ],
      "metadata": {
        "id": "XYD0xk1NUVmO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMm0P-RQdkvi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d8d4095-b964-42da-fbfc-b1623d3a5fcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.0+cu117\n",
            "Window size is 96 and horizon is 48 for sampling rate of 5 mins\n",
            "tsl version  : 0.9.5\n",
            "torch version: 2.0.0+cu117\n",
            "The chosen model is var_model\n",
            "Shape of data: (2605, 8)\n",
            "8\n",
            "Length of train, val and test splits are: 21, 9, 9\n",
            "Metadata of None is: {'n_channels': 1, 'horizon': 48, 'n_nodes': 8, 'window': 96}\n",
            "\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
            "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "2025-05-15 05:00:11.288135: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-15 05:00:11.306678: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747285211.327778   33556 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747285211.334193   33556 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-15 05:00:11.355043: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name          | Type             | Params\n",
            "---------------------------------------------------\n",
            "0 | loss_fn       | MaskedMAE        | 0     \n",
            "1 | train_metrics | MetricCollection | 0     \n",
            "2 | val_metrics   | MetricCollection | 0     \n",
            "3 | test_metrics  | MetricCollection | 0     \n",
            "4 | model         | VARModel         | 12.7 K\n",
            "---------------------------------------------------\n",
            "12.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "12.7 K    Total params\n",
            "0.051     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Metric val_loss improved. New best score: 2.641\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Metric val_loss improved by 0.099 >= min_delta = 0.0. New best score: 2.542\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Metric val_loss improved by 0.060 >= min_delta = 0.0. New best score: 2.482\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Metric val_loss improved by 0.029 >= min_delta = 0.0. New best score: 2.453\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Predictor\n",
            "Monitored metric val_loss did not improve in the last 5 records. Best score: 2.453. Signaling Trainer to stop.\n",
            "Predictor\n",
            "Restoring states from the checkpoint path at checkpoint/VARModel/best-19-2.45.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from the checkpoint at checkpoint/VARModel/best-19-2.45.ckpt\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m Runningstage.validating \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                           \u001b[0m┃\n",
            "┃\u001b[1m \u001b[0m\u001b[1m         metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.4529550075531006    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m         val_mae         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.4529550075531006    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m      val_mae_at_30      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.905030369758606    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m        val_mape         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.35458192229270935   \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Restoring states from the checkpoint path at checkpoint/VARModel/best-19-2.45.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from the checkpoint at checkpoint/VARModel/best-19-2.45.ckpt\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m  Runningstage.testing   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                           \u001b[0m┃\n",
            "┃\u001b[1m \u001b[0m\u001b[1m         metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    2.542750835418701    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m        test_mae         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    2.542750835418701    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m     test_mae_at_30      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.107581615447998    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m        test_mape        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2461915910243988    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Restoring states from the checkpoint path at checkpoint/VARModel/best-19-2.45.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from the checkpoint at checkpoint/VARModel/best-19-2.45.ckpt\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n",
            "Figure(1000x500)\n",
            "MAE at validation dataset is: 2.4529550075531006\n",
            "\n",
            "real\t0m18.431s\n",
            "user\t0m43.683s\n",
            "sys\t0m4.674s\n",
            "Processing root folder: save_inference_result/VAR_5mins_8Nodes\n",
            "Processing model: VARModel\n",
            "Metrics saved in save_inference_result/VAR_5mins_8Nodes\n",
            "All folders and subfolders processed successfully.\n",
            "DONE: Iteration: 1/45 \t Model: VAR\tSampling: 5\tNodes: 8\n",
            "2.0.0+cu117\n",
            "Window size is 32 and horizon is 16 for sampling rate of 15 mins\n",
            "tsl version  : 0.9.5\n",
            "torch version: 2.0.0+cu117\n",
            "The chosen model is var_model\n",
            "Shape of data: (869, 8)\n",
            "8\n",
            "Length of train, val and test splits are: 7, 3, 3\n",
            "Metadata of None is: {'n_channels': 1, 'horizon': 16, 'n_nodes': 8, 'window': 32}\n",
            "\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
            "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "2025-05-15 05:00:30.361355: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-15 05:00:30.380509: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747285230.401988   33714 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747285230.408480   33714 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-15 05:00:30.429575: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory checkpoint/VARModel exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name          | Type             | Params\n",
            "---------------------------------------------------\n",
            "0 | loss_fn       | MaskedMAE        | 0     \n",
            "1 | train_metrics | MetricCollection | 0     \n",
            "2 | val_metrics   | MetricCollection | 0     \n",
            "3 | test_metrics  | MetricCollection | 0     \n",
            "4 | model         | VARModel         | 4.2 K \n",
            "---------------------------------------------------\n",
            "4.2 K     Trainable params\n",
            "0         Non-trainable params\n",
            "4.2 K     Total params\n",
            "0.017     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Benchmark_STGNN_for_STLF/SpatioTemporal_TS_with_Graph.py\", line 551, in <module>\n",
            "    main()\n",
            "  File \"/content/Benchmark_STGNN_for_STLF/SpatioTemporal_TS_with_Graph.py\", line 547, in main\n",
            "    validation_mae = stgraph.run(experiment_id=experiment_id, method=chosen_graph_creation_method)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Benchmark_STGNN_for_STLF/SpatioTemporal_TS_with_Graph.py\", line 258, in run\n",
            "    validation_result = trainer.validate(predictor, self.datamodule, ckpt_path='best')\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 633, in validate\n",
            "    return call._call_and_handle_interrupt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\", line 43, in _call_and_handle_interrupt\n",
            "    return trainer_fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 673, in _validate_impl\n",
            "    ckpt_path = self._checkpoint_connector._select_ckpt_path(\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 108, in _select_ckpt_path\n",
            "    ckpt_path = self._parse_ckpt_path(\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 175, in _parse_ckpt_path\n",
            "    raise ValueError(\n",
            "ValueError: `.validate(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.\n",
            "^C\n",
            "object address  : 0x7b58465507c0\n",
            "object refcount : 2\n",
            "object type     : 0x9d5ea0\n",
            "object type name: KeyboardInterrupt\n",
            "object repr     : KeyboardInterrupt()\n",
            "lost sys.stderr\n",
            "^C\n",
            "DONE: Iteration: 2/45 \t Model: VAR\tSampling: 15\tNodes: 8\n",
            "2.0.0+cu117\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Benchmark_STGNN_for_STLF/SpatioTemporal_TS_with_Graph.py\", line 13, in <module>\n",
            "    import pandas as pd\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\", line 49, in <module>\n",
            "    from pandas.core.api import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/api.py\", line 47, in <module>\n",
            "    from pandas.core.groupby import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/__init__.py\", line 1, in <module>\n",
            "    from pandas.core.groupby.generic import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/generic.py\", line 73, in <module>\n",
            "    from pandas.core.groupby.groupby import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/groupby.py\", line 130, in <module>\n",
            "    from pandas.core.groupby.indexing import (\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 936, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1032, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1130, in get_data\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "^C\n",
            "DONE: Iteration: 3/45 \t Model: VAR\tSampling: 30\tNodes: 8\n",
            "^C\n",
            "Processing root folder: save_inference_result/VAR_45mins_8Nodes\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Benchmark_STGNN_for_STLF/tools/evaluate_test_results.py\", line 76, in <module>\n",
            "    process_folder(root_folder, df, df_agg)\n",
            "  File \"/content/Benchmark_STGNN_for_STLF/tools/evaluate_test_results.py\", line 32, in process_folder\n",
            "    raise ValueError(\"No 'y.npy' file found in the specified files.\")\n",
            "ValueError: No 'y.npy' file found in the specified files.\n",
            "DONE: Iteration: 4/45 \t Model: VAR\tSampling: 45\tNodes: 8\n",
            "^C\n",
            "Processing root folder: save_inference_result/VAR_60mins_8Nodes\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Benchmark_STGNN_for_STLF/tools/evaluate_test_results.py\", line 76, in <module>\n",
            "    process_folder(root_folder, df, df_agg)\n",
            "  File \"/content/Benchmark_STGNN_for_STLF/tools/evaluate_test_results.py\", line 32, in process_folder\n",
            "    raise ValueError(\"No 'y.npy' file found in the specified files.\")\n",
            "ValueError: No 'y.npy' file found in the specified files.\n",
            "DONE: Iteration: 5/45 \t Model: VAR\tSampling: 60\tNodes: 8\n"
          ]
        }
      ],
      "source": [
        "!rm -rf save_inference_result\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "from google.colab import files\n",
        "# 2) Define your experiments\n",
        "nodes = [8, 16, 25]\n",
        "models = {\"var_model\": \"VAR\", \"rnn_model\": \"GRU\", \"tf_model\": \"Transformer\"}\n",
        "samplings = [5,15,30,45,60]\n",
        "count = 0\n",
        "total = len(nodes) * len(models) * len(samplings)\n",
        "# 3) Run experiments and async download snapshots\n",
        "for model, name in models.items():\n",
        "    for node in nodes:\n",
        "        for sampling in samplings:\n",
        "          !time python3 Benchmark_STGNN_for_STLF/SpatioTemporal_TS_with_Graph.py \\\n",
        "          {model} {name}_{sampling}mins_{node}Nodes \\\n",
        "          --window 8 \\\n",
        "          --hidden_dimension 64 \\\n",
        "          --learning_rate 0.001 \\\n",
        "          --batch_size 64 \\\n",
        "          --horizon 4 \\\n",
        "          --sampling {sampling} \\\n",
        "          --nodes {node}\n",
        "          !python Benchmark_STGNN_for_STLF/tools/evaluate_test_results.py \\\n",
        "          save_inference_result/{name}_{sampling}mins_{node}Nodes\n",
        "          count += 1\n",
        "          print(f\"DONE: Iteration: {count}/{total} \\t Model: {name}\\tSampling: {sampling}\\tNodes: {node}\")\n",
        "!zip -r save_inference_result_Temperature_Temporal_Models.zip save_inference_result\n",
        "from google.colab import files\n",
        "files.download('save_inference_result_Temperature_Temporal_Models.zip')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}