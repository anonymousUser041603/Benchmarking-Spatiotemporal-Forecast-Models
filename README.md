# No One-Size-Fits-All: Uncovering Spatioâ€‘Temporal Forecasting Tradeâ€‘offs with Graph Neural Networks and Foundation Models

## ðŸ“‘ Table of Contents
- [Abstract](#abstract)  
- [Background & Motivation](#background--motivation)  
- [Model Overview](#model-overview)  
  - [Classical Baselines](#classical-baselines)  
  - [Time Series Foundation Models (TSFMs)](#time-series-foundation-models-tsfms)  
  - [Spatioâ€‘Temporal GNNs (STGNNs)](#spatio-temporal-gnns-stgnns)  
- [Dataset & Preprocessing](#dataset--preprocessing)  
- [Methodology](#methodology)  
  - [Experimental Design](#experimental-design)  
- [Implementation](#implementation)  

---

## Abstract
Modern IoT deployments produce highâ€‘volume spatiotemporal data for forecasting tasks. We benchmark classical (VAR, GRU, Transformer), Spatioâ€‘Temporal GNNs (GRUGCN, Tâ€‘GCN), and Time Series Foundation Models (Moirai, TimesFM) on temperature forecasting in a 25â€‘node wireless sensor network. By varying sampling rates (5â€“60â€¯min) and node counts (8, 16, 25), we uncover tradeâ€‘offs: STGNNs excel under sparse coverage and moderate frequency, TSFMs shine at high frequency but degrade with reduced spatial context. Our study guides model and deployment choices in largeâ€‘scale sensing systems.

---

## Background & Motivation
Environmental sensing and smartâ€‘grid applications rely on dense IoT networks streaming highâ€‘frequency data. While edge sampling strategies aim to reduce data volume, their impact on centralized forecasting models is underâ€‘studied. Different architectures (classical, RNNs, Transformers, STGNNs, TSFMs) exhibit unique sensitivities to temporal resolution and spatial coverage. We ask:  
> **Do graphâ€‘based STGNNs inherently outperform nonâ€‘graph alternatives across sampling frequencies and node densities?**

---

## Model Overview

### Classical Baselines
- **VAR**: Multivariate autoregression leveraging all series jointly, no training required.  
- **GRU**: Gated Recurrent Units for sequential temporal modeling.  
- **Transformer**: Selfâ€‘attention model for longâ€‘range temporal dependencies.

### Time Series Foundation Models (TSFMs)
| Attribute                  | TimesFM (200â€¯M)      | Moiraiâ€‘Small (14â€¯M)     |
|----------------------------|----------------------|-------------------------|
| Zeroâ€‘shot                  | âœ“                    | âœ“                       |
| Multivariate               | âœ— (uni + covariates) | âœ“                       |
| Probabilistic Forecasting  | âœ— (pointâ€‘wise)       | âœ“                       |
| Pretraining Corpus         | 100â€¯B timeâ€‘points    | 27â€¯B observations       |
| Architecture               | Decoderâ€‘only         | Encoderâ€‘only            |
| Tokenization               | Fixed patches        | Multiâ€‘resolution patches|

### Spatioâ€‘Temporal GNNs (STGNNs)
- **GRUGCN**  
  1. GRU perâ€‘node for temporal patterns  
  2. GCN aggregation of neighbor states  
- **Tâ€‘GCN**  
  1. GCN on raw features per time step  
  2. GRU for sequence modeling  

Graphs encode sensor similarity via Pearson correlation; see [Graph Construction](#graph-construction).

---

## Dataset & Preprocessing
- **Source**: IoBT wireless weather network (25 towers, New Mexico)  
- **Feature**: Temperature (Â°C) sampled every 5â€¯min over 9â€¯days  
- **Splits**: 5â€¯days train / 2â€¯days validation / 2â€¯days test  
- **Task**: Forecast horizon \(H=4\)â€¯h from context window \(W=8\)â€¯h  
  $$
    W = 8\text{â€¯h},\quad H = 4\text{â€¯h}
  $$
- **Downsampling**: \{5,â€¯15,â€¯30,â€¯45,â€¯60\}â€¯min  

### Preprocessing Steps
1. **Zâ€‘score normalization**  
2. **Window aggregation** to fixed \(W\) per node  
3. **Graph thresholding** by topâ€‘\(p\)% Pearson correlations  

---

## Methodology

### Experimental Design
- **Variables**  
  - Sampling rate: 5â€“60â€¯min  
  - Node count: 8,â€¯16,â€¯25  
- **Metrics**: MAE, RMSE, MAPE  

Hereâ€™s the corrected Markdown with properly formatted equations:
## Implementation
